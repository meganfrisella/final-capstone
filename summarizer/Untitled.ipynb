{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import reuters\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def strip_punc(corpus):\n",
    "    \"\"\" Removes all punctuation from a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            the corpus with all punctuation removed\"\"\"\n",
    "    # substitute all punctuation marks with \"\"\n",
    "    return punc_regex.sub('', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_counter(doc):\n",
    "    \"\"\" \n",
    "    Produce word-count of document, removing all punctuation\n",
    "    and making all the characters lower-cased.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    collections.Counter\n",
    "        lower-cased word -> count\"\"\"\n",
    "    doc = sorted(strip_punc(doc).lower().split())\n",
    "    return Counter(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(counters, k=None, stop_words=None):\n",
    "    \"\"\" \n",
    "    [word, word, ...] -> sorted list of top-k unique words\n",
    "    Excludes words included in `stop_words`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    counters : Iterable[Iterable[str]]\n",
    "    \n",
    "    k : Optional[int]\n",
    "        If specified, only the top-k words are returned\n",
    "    \n",
    "    stop_words : Optional[Collection[str]]\n",
    "        A collection of words to be ignored when populating the vocabulary\n",
    "    \"\"\"\n",
    "    unique = Counter()\n",
    "    for c in counters:\n",
    "        unique.update(c)\n",
    "    if stop_words is not None:\n",
    "        for word in stop_words:\n",
    "            del unique[word]\n",
    "    if k is not None:\n",
    "        unique = set(unique.most_common(k))\n",
    "        return sorted(list(unique))\n",
    "    return sorted(list(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf(counter, vocab):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    counter : collections.Counter\n",
    "        The word -> count mapping for a document.\n",
    "    vocab : Sequence[str]\n",
    "        Ordered list of words that we care about.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The TF descriptor for the document, whose components represent\n",
    "        the frequency with which each term in the vocab occurs\n",
    "        in the given document.\"\"\"\n",
    "    total = 0.0\n",
    "    for key in counter:\n",
    "        if key in vocab:\n",
    "            total += counter[key]\n",
    "    tf = []\n",
    "    for word in vocab:\n",
    "        if counter[word] is None:\n",
    "            tf.append(0)\n",
    "        else:\n",
    "            tf.append(1.0 * counter[word] / total)\n",
    "    return np.array(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_idf(vocab, counters):\n",
    "    \"\"\" \n",
    "    Given the vocabulary, and the word-counts for each document, computes\n",
    "    the inverse document frequency (IDF) for each term in the vocabulary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : Sequence[str]\n",
    "        Ordered list of words that we care about.\n",
    "\n",
    "    counters : Iterable[collections.Counter]\n",
    "        The word -> count mapping for each document.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array whose entries correspond to those in `vocab`, storing\n",
    "        the IDF for each term `t`: \n",
    "                           log10(N / nt)\n",
    "        Where `N` is the number of documents, and `nt` is the number of \n",
    "        documents in which the term `t` occurs.\n",
    "    \"\"\"\n",
    "    N = 1.0 * len(counters)\n",
    "    idf = []\n",
    "    for i in range(len(vocab)):\n",
    "        word = vocab[i]\n",
    "        docs = 0.0\n",
    "        for countmap in counters:\n",
    "            if word in countmap:\n",
    "                docs += 1.0\n",
    "        idf.append(N/docs)\n",
    "    return np.array(np.log10(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** BEGIN ARTICLE: ** \"THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\n",
      "  Thailand's trade deficit widened to 4.5\n",
      "  billion baht in the first quarter of 1987 from 2.1 billion a\n",
      "  year ago, the Business Economics Department said.\n",
      "      It said Janunary/March imports rose to 65.1 billion baht\n",
      "  from 58.7 billion. Thailand's improved business climate this\n",
      "  year resulted in a 27 pct increase in imports of raw materials\n",
      "  and semi-finished products.\n",
      "      The country's oil import bill, however, fell 23 pct in the\n",
      "  first quarte [...]\"\n",
      "** BEGIN ARTICLE: ** \"CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of China's grain\n",
      "  stocks, the China Daily said.\n",
      "      It also said that each year 1.575 mln tonnes, or 25 pct, of\n",
      "  China's fruit output are left to rot, and 2.1 mln tonnes, or up\n",
      "  to 30 pct, of its vegetables. The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods.\n",
      "      It said the government had launched a national programme  [...]\"\n"
     ]
    }
   ],
   "source": [
    "doc_1 = '** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[0])[:500] + ' [...]\\\"'\n",
    "doc_2 = '** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[1])[:500] + ' [...]\\\"'\n",
    "doc_3 = '** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[2])[:500] + ' [...]\\\"'\n",
    "doc_4 = '** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[3])[:500] + ' [...]\\\"'\n",
    "doc_5 = '** BEGIN ARTICLE: ** \\\"' + reuters.raw(reuters.fileids()[4])[:500] + ' [...]\\\"'\n",
    "print(doc_4)\n",
    "print(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \"\"\"We present a database of well-determined orbital parameters of exoplanets, and their host stars’\n",
    "properties. This database comprises spectroscopic orbital elements measured for 427 planets orbiting 363 stars from\n",
    "radial velocity and transit measurements as reported in the literature. We have also compiled fundamental transit\n",
    "parameters, stellar parameters, and the method used for the planets discovery. This Exoplanet Orbit Database includes all planets with robust, well measured orbital parameters reported in peer-reviewed articles. The database is\n",
    "available in a searchable, filterable, and sortable form online through the Exoplanets Data Explorer table, and the\n",
    "data can be plotted and explored through the Exoplanet Data Explorer plotter. We use the Data Explorer to generate\n",
    "publication-ready plots, giving three examples of the signatures of exoplanet migration and dynamical evolution:\n",
    "We illustrate the character of the apparent correlation between mass and period in exoplanet orbits, the different\n",
    "selection biases between radial velocity and transit surveys, and that the multiplanet systems show a distinct\n",
    "semimajor-axis distribution from apparently singleton systems.\"\"\"\n",
    "\n",
    "doc_2 = \"\"\"For the Exoplanet Orbit Database, we have dropped the \n",
    "200 pc limit from the old catalog and now include all robustly\n",
    "detected planets appearing in the peer-reviewed literature with\n",
    "well-determined orbital parameters. We have retained the generous upper mass limit of 24 Jupiter masses in our definition of a\n",
    "“planet,” for the same reasons as in the catalog: at the moment,\n",
    "any mass limit is arbitrary and will serve little practical function,\n",
    "both because of the sin i ambiguity in radial velocity masses and\n",
    "because of the lack of physical motivation.13 We therefore err on\n",
    "the side of inclusiveness by admitting the long high-mass tail of\n",
    "the exoplanet population at the risk of having a few bona fide\n",
    "brown dwarfs in the sample.\"\"\"\n",
    "\n",
    "doc_3 = \"\"\"We have opted to use these classical SB1 orbital parameters,\n",
    "rather than using mean longitude at epoch, because they are\n",
    "more frequently reported in the literature and the latter is\n",
    "trivially computed from the former. In those cases (especially\n",
    "for multiplanet systems or transiting systems) where the phase\n",
    "of a planet is reported as the mean anomaly at epoch, or epoch\n",
    "of transit center, or in some similar way, we have converted the\n",
    "quantities to ω and T0 for consistency. We recognize that for\n",
    "circular orbits the uncertainty in mean longitude is better behaved than those in T0 and ω, and we note that the uncertainty\n",
    "in mean longitude can be estimated from the period uncertainty\n",
    "and the span of the observations. We plan to incorporate mean\n",
    "longitude at epoch, transit time predictions, and robust uncertainties for these quantities in the future, but in the meantime,\n",
    "any application requiring more precision should calculate the\n",
    "quantity explicitly from the radial velocities or from the source\n",
    "article.\"\"\"\n",
    "\n",
    "doc_4 = \"\"\"The EOD can be explored and displayed using the Exoplanet\n",
    "Data Explorer table and plotter.\n",
    "The Table Explorer allows for the user to dynamically create\n",
    "a sorted table of planets and selected properties, including a\n",
    "choice of units and parameter uncertainties. Once a table has\n",
    "been generated, it may be exported as a custom text file. References are linked to their corresponding URLs; we provide columns for links to SIMBAD, NStED, and Exoplanet Transit\n",
    "Database; and planets are linked to “one-up” planet pages that\n",
    "contain all fields and values for a given set of planets. Both\n",
    "pages as illustrated in Figure 1.\n",
    "These one-up pages include a link to the publicly available\n",
    "velocities of each star, stored at NStED, and a plot showing\n",
    "these published velocities as a function of time or phase (as appropriate), along with a velocity curve generated from the listed\n",
    "orbital solution. Note that we have not attempted to fit the\n",
    "velocities and generate our own solution; we solve only for\n",
    "the velocity offset γ and simply overplot the solution and data.\n",
    "This serves as a check on the accuracy of our transcription of\n",
    "orbital elements\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '200', '24', '363', '427', 'accuracy', 'admitting', 'allows', 'along', 'also', 'ambiguity', 'anomaly', 'apparent', 'apparently', 'appearing', 'application', 'appropriate', 'arbitrary', 'article', 'articles', 'attempted', 'available', 'behaved', 'better', 'biases', 'bona', 'brown', 'calculate', 'cases', 'catalog', 'center', 'character', 'check', 'choice', 'circular', 'classical', 'columns', 'compiled', 'comprises', 'computed', 'consistency', 'contain', 'converted', 'correlation', 'corresponding', 'create', 'curve', 'custom', 'data', 'database', 'definition', 'detected', 'different', 'discovery', 'displayed', 'distinct', 'distribution', 'dropped', 'dwarfs', 'dynamical', 'dynamically', 'elements', 'eod', 'epoch', 'err', 'especially', 'estimated', 'evolution', 'examples', 'exoplanet', 'exoplanets', 'explicitly', 'explored', 'explorer', 'exported', 'fide', 'fields', 'figure', 'file', 'filterable', 'fit', 'form', 'former', 'frequently', 'function', 'fundamental', 'future', 'generate', 'generated', 'generous', 'given', 'giving', 'highmass', 'host', 'illustrate', 'illustrated', 'include', 'includes', 'including', 'inclusiveness', 'incorporate', 'jupiter', 'lack', 'latter', 'limit', 'link', 'linked', 'links', 'listed', 'literature', 'little', 'long', 'longitude', 'mass', 'masses', 'may', 'mean', 'meantime', 'measured', 'measurements', 'method', 'migration', 'moment', 'motivation13', 'multiplanet', 'note', 'nsted', 'observations', 'offset', 'old', 'oneup', 'online', 'opted', 'orbit', 'orbital', 'orbiting', 'orbits', 'overplot', 'pages', 'parameter', 'parameters', 'pc', 'peerreviewed', 'period', 'phase', 'physical', 'plan', 'planet', 'planets', 'plot', 'plots', 'plotted', 'plotter', 'population', 'practical', 'precision', 'predictions', 'present', 'properties', 'provide', 'publicationready', 'publicly', 'published', 'quantities', 'quantity', 'radial', 'rather', 'reasons', 'recognize', 'references', 'reported', 'requiring', 'retained', 'risk', 'robust', 'robustly', 'sample', 'sb1', 'searchable', 'selected', 'selection', 'semimajoraxis', 'serve', 'serves', 'set', 'show', 'showing', 'side', 'signatures', 'simbad', 'similar', 'simply', 'sin', 'singleton', 'solution', 'solve', 'sortable', 'sorted', 'source', 'span', 'spectroscopic', 'star', 'stars', 'stars’', 'stellar', 'stored', 'surveys', 'systems', 't0', 'table', 'tail', 'text', 'therefore', 'three', 'time', 'transcription', 'transit', 'transiting', 'trivially', 'uncertainties', 'uncertainty', 'units', 'upper', 'urls', 'use', 'used', 'user', 'using', 'values', 'velocities', 'velocity', 'way', 'well', 'welldetermined', 'γ', 'ω', '“oneup”', '“planet”']\n",
      "(4, 238)\n"
     ]
    }
   ],
   "source": [
    "docs = [doc_1, doc_2, doc_3, doc_4]\n",
    "counter = [to_counter(i) for i in docs]\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('english')\n",
    "vocab = to_vocab(counter, stop_words = stops)\n",
    "print(vocab)\n",
    "tf = tuple(to_tf(to_counter(i), vocab) for i in docs)\n",
    "\n",
    "idf = to_idf(vocab, counter)\n",
    "\n",
    "tf_idf = tf * idf\n",
    "print(tf_idf.shape)\n",
    "cos_sim = cosine_similarity(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1_split = \" \".join(doc_1.split(\"\\n\")).split(\". \")\n",
    "doc_2_split = \" \".join(doc_2.split(\"\\n\")).split(\". \")\n",
    "doc_3_split = \" \".join(doc_3.split(\"\\n\")).split(\". \")\n",
    "doc_4_split = \" \".join(doc_4.split(\"\\n\")).split(\". \")\n",
    "split_docs = [doc_1_split, doc_2_split, doc_3_split, doc_4_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We present a database of well-determined orbital parameters of exoplanets, and their host stars’ properties', 'This database comprises spectroscopic orbital elements measured for 427 planets orbiting 363 stars from radial velocity and transit measurements as reported in the literature', 'We have also compiled fundamental transit parameters, stellar parameters, and the method used for the planets discovery', 'This Exoplanet Orbit Database includes all planets with robust, well measured orbital parameters reported in peer-reviewed articles', 'The database is available in a searchable, filterable, and sortable form online through the Exoplanets Data Explorer table, and the data can be plotted and explored through the Exoplanet Data Explorer plotter', 'We use the Data Explorer to generate publication-ready plots, giving three examples of the signatures of exoplanet migration and dynamical evolution: We illustrate the character of the apparent correlation between mass and period in exoplanet orbits, the different selection biases between radial velocity and transit surveys, and that the multiplanet systems show a distinct semimajor-axis distribution from apparently singleton systems.']\n"
     ]
    }
   ],
   "source": [
    "print(doc_1_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "present\n",
      "a\n",
      "database\n",
      "of\n",
      "well-determined\n",
      "orbital\n",
      "parameters\n",
      "of\n",
      "exoplanets,\n",
      "and\n",
      "their\n",
      "host\n",
      "stars’\n",
      "properties\n",
      "This\n",
      "database\n",
      "comprises\n",
      "spectroscopic\n",
      "orbital\n",
      "elements\n",
      "measured\n",
      "for\n",
      "427\n",
      "planets\n",
      "orbiting\n",
      "363\n",
      "stars\n",
      "from\n",
      "radial\n",
      "velocity\n",
      "and\n",
      "transit\n",
      "measurements\n",
      "as\n",
      "reported\n",
      "in\n",
      "the\n",
      "literature\n",
      "We\n",
      "have\n",
      "also\n",
      "compiled\n",
      "fundamental\n",
      "transit\n",
      "parameters,\n",
      "stellar\n",
      "parameters,\n",
      "and\n",
      "the\n",
      "method\n",
      "used\n",
      "for\n",
      "the\n",
      "planets\n",
      "discovery\n",
      "This\n",
      "Exoplanet\n",
      "Orbit\n",
      "Database\n",
      "includes\n",
      "all\n",
      "planets\n",
      "with\n",
      "robust,\n",
      "well\n",
      "measured\n",
      "orbital\n",
      "parameters\n",
      "reported\n",
      "in\n",
      "peer-reviewed\n",
      "articles\n",
      "The\n",
      "database\n",
      "is\n",
      "available\n",
      "in\n",
      "a\n",
      "searchable,\n",
      "filterable,\n",
      "and\n",
      "sortable\n",
      "form\n",
      "online\n",
      "through\n",
      "the\n",
      "Exoplanets\n",
      "Data\n",
      "Explorer\n",
      "table,\n",
      "and\n",
      "the\n",
      "data\n",
      "can\n",
      "be\n",
      "plotted\n",
      "and\n",
      "explored\n",
      "through\n",
      "the\n",
      "Exoplanet\n",
      "Data\n",
      "Explorer\n",
      "plotter\n",
      "We\n",
      "use\n",
      "the\n",
      "Data\n",
      "Explorer\n",
      "to\n",
      "generate\n",
      "publication-ready\n",
      "plots,\n",
      "giving\n",
      "three\n",
      "examples\n",
      "of\n",
      "the\n",
      "signatures\n",
      "of\n",
      "exoplanet\n",
      "migration\n",
      "and\n",
      "dynamical\n",
      "evolution:\n",
      "We\n",
      "illustrate\n",
      "the\n",
      "character\n",
      "of\n",
      "the\n",
      "apparent\n",
      "correlation\n",
      "between\n",
      "mass\n",
      "and\n",
      "period\n",
      "in\n",
      "exoplanet\n",
      "orbits,\n",
      "the\n",
      "different\n",
      "selection\n",
      "biases\n",
      "between\n",
      "radial\n",
      "velocity\n",
      "and\n",
      "transit\n",
      "surveys,\n",
      "and\n",
      "that\n",
      "the\n",
      "multiplanet\n",
      "systems\n",
      "show\n",
      "a\n",
      "distinct\n",
      "semimajor-axis\n",
      "distribution\n",
      "from\n",
      "apparently\n",
      "singleton\n",
      "systems.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc_1_split:\n",
    "    sentence = sentence.split(\" \")\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02930867794824781, 0.07731802790630285, 0.04683068263156166, 0.04232953871911201, 0.04731325359804639, 0.1397959515346763]\n",
      "We present a database of well-determined orbital parameters of exoplanets, and their host stars’ properties\n",
      "\n",
      "[0.1254058111337211, 0.38063642235275746]\n",
      "For the Exoplanet Orbit Database, we have dropped the  200 pc limit from the old catalog and now include all robustly detected planets appearing in the peer-reviewed literature with well-determined orbital parameters\n",
      "\n",
      "[0.1410253484370681, 0.16824667412311894, 0.2606479230749105, 0.16609921974439107]\n",
      "We have opted to use these classical SB1 orbital parameters, rather than using mean longitude at epoch, because they are more frequently reported in the literature and the latter is trivially computed from the former\n",
      "\n",
      "[0.02605067270169068, 0.07596698174446567, 0.040523268647074395, 0.10272713727462676, 0.028945191890767424, 0.13265603152015149, 0.07766016692769338, 0.02605067270169068]\n",
      "The EOD can be explored and displayed using the Exoplanet Data Explorer table and plotter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in split_docs:\n",
    "    doc_stats = []\n",
    "    for sentence in doc:\n",
    "        sentence = sentence.split(\" \")\n",
    "        sentence_tfidf = 0\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                word_stat = tf_idf[split_docs.index(doc),vocab.index(word)]\n",
    "                sentence_tfidf += word_stat\n",
    "        doc_stats.append(sentence_tfidf)\n",
    "    print(doc_stats)\n",
    "    print(doc[doc_stats.index(min(doc_stats))])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building term-document matrix... [process started: 2019-07-31 15:10:54.120645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done! [process finished: 2019-07-31 15:12:01.222208]\n"
     ]
    }
   ],
   "source": [
    "import datetime, re, sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "token_dict = {}\n",
    "for article in reuters.fileids():\n",
    "    token_dict[article] = reuters.raw(article)\n",
    "        \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
    "print ('building term-document matrix... [process started: ' + str(datetime.datetime.now()) + ']')\n",
    "sys.stdout.flush()\n",
    "\n",
    "tdm = tfidf.fit_transform(token_dict.values()) # this can take some time (about 60 seconds on my machine)\n",
    "print ('done! [process finished: ' + str(datetime.datetime.now()) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM contains 25827 terms and 10788 documents\n",
      "first term: 'd\n",
      "last term: zzzz\n",
      "random term: carlson\n",
      "random term: marriag\n",
      "random term: caspian\n",
      "random term: dual\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print ('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')\n",
    "\n",
    "print ('first term: ' + feature_names[0])\n",
    "print ('last term: ' + feature_names[len(feature_names) - 1])\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print ('random term: ' + feature_names[randint(1,len(feature_names) - 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SUMMARY ***\n",
      "AUSTRALIA SAID TO RELY TOO MUCH ON OIL TAXES\n",
      "  The government's\n",
      "  over-reliance on revenue from crude oil is adversely affecting\n",
      "  Australia's economic performance, Australian Petroleum\n",
      "  Exploration Association (APEA) chairman Dennis Benbow said.\n",
      "Domestic oil output from existing fields is expected to\n",
      "  fall to 280,000 barrels per day (bpd) in fiscal 1992/93 from\n",
      "  546,000 bpd in 1985/86, reflecting mainly the decline of the\n",
      "  Bass Strait fields, he said.\n",
      "Bass Strait reserves are now two-thirds depleted, with the\n",
      "  three largest fields 80 pct depleted, he said.\n",
      "\n",
      "*** ORIGINAL ***\n",
      "AUSTRALIA SAID TO RELY TOO MUCH ON OIL TAXES\n",
      "  The government's\n",
      "  over-reliance on revenue from crude oil is adversely affecting\n",
      "  Australia's economic performance, Australian Petroleum\n",
      "  Exploration Association (APEA) chairman Dennis Benbow said.\n",
      "      Over one-third of Australia's indirect tax income is\n",
      "  derived from oil at a time of falling domestic output and weak\n",
      "  crude prices, he told the APEA annual conference here.\n",
      "      This dependence on oil-generated revenue distorts the\n",
      "  country's economic performance directly by acting as a\n",
      "  disincentive to new exploration and indirectly by affecting\n",
      "  trading competitiveness through high energy costs, he said.\n",
      "      Australia's medium-term liquid fuel self-sufficiency\n",
      "  position is posing a major economic threat, yet the\n",
      "  government's response has been to load new tax burdens on the\n",
      "  oil industry, Benbow said.\n",
      "      Domestic oil output from existing fields is expected to\n",
      "  fall to 280,000 barrels per day (bpd) in fiscal 1992/93 from\n",
      "  546,000 bpd in 1985/86, reflecting mainly the decline of the\n",
      "  Bass Strait fields, he said.\n",
      "      Bass Strait reserves are now two-thirds depleted, with the\n",
      "  three largest fields 80 pct depleted, he said.\n",
      "      By 1992/93, Bass Strait output is expected to be just over\n",
      "  half the 1985/86 level, assuming a number of so far undeveloped\n",
      "  fields are brought on stream and enhanced recovery from\n",
      "  existing fields goes ahead, Benbow said.\n",
      "      Government projections of output from as yet undiscovered\n",
      "  fields range from 40,000 to 130,000 bpd, he said.\n",
      "      Australian liquid fuel demand is forecast to rise to\n",
      "  680,000 bpd in 1992/93 from 565,000 in 1985/86, implying a\n",
      "  crude oil gap of between 270,000 and 360,000 bpd in five years\n",
      "  time, he said.\n",
      "      At present world oil prices and the current value of the\n",
      "  Australian dollar, annual oil imports in 1992/93 would cost\n",
      "  between 3.2 billion and 3.6 billion dlrs, Benbow said.\n",
      "      Despite intensive exploration in the early 1980's, the\n",
      "  addition to reserves has been inadequate, he said.\n",
      "      For example, the 409 mln barrels discovered in the five\n",
      "  years 1980-84 represent about two years' consumption, he said.\n",
      "      He called on the government to review its tax policies to\n",
      "  restore incentive to exploration.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "\n",
    "\n",
    "article_id = randint(0, tdm.shape[0] - 1)\n",
    "article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "\n",
    "print ('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    print (summary_sentence[1])\n",
    "\n",
    "print ('\\n*** ORIGINAL ***')\n",
    "print (article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2 1 6 5 4]\n",
      "[0.1  0.2  0.3  0.6  0.7  0.74 0.9 ]\n",
      "0.28217811714574603\n",
      "[0.1  0.1  0.3  0.1  0.04 0.16]\n",
      "(array([2], dtype=int64),)\n",
      "1\n",
      "[1 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([.1, .6, .3, .2, .9, .74, .7])\n",
    "sort_ind = np.argsort(arr)\n",
    "print(sort_ind)\n",
    "sort_arr = np.array(sorted(arr))\n",
    "print(sort_arr)\n",
    "std = np.std(np.array(arr))\n",
    "print(std)\n",
    "diff = sort_arr[1:] - sort_arr[:-1]\n",
    "print(diff)\n",
    "where = np.where(diff>std)\n",
    "print(where)\n",
    "if len(where[0]) != 0:\n",
    "    ind = sort_ind[where[0][0] + 1]\n",
    "    print(ind)\n",
    "    indices = np.where(arr >= arr[ind])\n",
    "print(indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "print(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
